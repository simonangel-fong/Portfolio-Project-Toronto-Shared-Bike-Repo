pipeline {
  agent any

  options {
    disableConcurrentBuilds()   // only one instance
    timestamps()    // show timestamp
    timeout(time: 30, unit: 'MINUTES')    // timeout for pipeline run
    // skipDefaultCheckout(true)   // Prevents auto SCM checkout

    // discard old builds
    buildDiscarder(
      logRotator(
        // maximum number of the last build logs
        numToKeepStr: '10',   
        // maximum number of the last sets of build artifacts
        artifactNumToKeepStr: '5'   
      )
    )
  }

  environment {
    GITHUB_URL = "https://github.com/simonangel-fong/Portfolio-Project-Toronto-Shared-Bike-Repo.git"
    GITHUB_BRANCH = "feature-dw-dev"
    POSTGRES_DB = "toronto_shared_bike"
    TEST_DATA="https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/test_data.zip"
    RAW_DATA="https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/data.zip"
  }

  stages {

    // stage('Clone GitHub Repository') {
    //   steps {
    //     cleanWs()
    //     checkout scmGit(
    //       userRemoteConfigs: [[url: "${env.GITHUB_URL}"]],
    //       branches: [[name: "${env.GITHUB_BRANCH}"]]
    //     )
    //   }
    // } 
 
    stage('Start PostgreSQL Database') {
      steps {
        withCredentials([
          string(credentialsId: 'postgres_user', variable: 'POSTGRES_USER'),
          string(credentialsId: 'postgres_password', variable: 'POSTGRES_PASSWORD'),
          ]) {
            dir("data-warehouse/postgresql"){
              echo "#################### Spin up PGDB ####################"
              sh '''
                set -Eeu pipefail

                pwd
                ls -l
                docker compose -f docker-compose.yaml down -v
                docker compose up -d --build

                # Wait until Postgres is ready
                until docker exec -t postgresql bash -lc 'pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB"'; do
                  echo "Waiting for PostgreSQL to become ready..."
                  sleep 5
                done
              '''

              echo "#################### Confirm PGDB ####################"
              sh '''
                docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}"
                docker logs --tail=100 postgresql || true
              '''
            }
        } 
      }
    }

    stage('Check PostgreSQL Database Objects') {
      steps {
        echo "#################### Check PostgreSQL Database ####################"
        sh 'docker exec postgresql bash /scripts/testing/object_check.sh'
      }
    }

    stage('Download CSV Files') {
      steps {
        dir("data-warehouse/postgresql"){
          sh '''
            ls -l
            mkdir -pv data 
            mkdir -pv export
          '''

          script {
            echo "Current branch: ${env.GIT_BRANCH}"

            if (env.GIT_BRANCH == 'origin/master') {
              echo "#################### Download Raw CSV Zip ####################"
              sh '''
                curl -o ./csv.zip $RAW_DATA
              '''
            } else {
              echo "#################### Download Test CSV Zip ####################"
              sh '''
                curl -o ./csv.zip $TEST_DATA
              '''
            }
          }

          echo "#################### Unzip CSV zip ####################"
          sh '''
            unzip -o ./csv.zip -d .
            ls -l ./data
            du -h ./data
            rm -fv ./csv.zip
          '''
        }
      }
    }

    // stage('Extract Data') {
    //   steps {
    //     echo "#################### Extract Data ####################"
    //     sh 'docker exec -t postgresql bash /scripts/etl/extract.sh'
    //   }
    // }

    // stage('Transform Data') {
    //   steps {
    //     sh 'docker exec -t postgresql bash /scripts/etl/transform.sh'
    //   }
    // }

    // stage('Load Data') {
    //   steps {
    //     sh 'docker exec -t postgresql bash /scripts/etl/load.sh'
    //   }
    // }

    // stage('Refresh Materialized Views') {
    //   steps {
    //     sh 'docker exec -t postgresql bash /scripts/mv/mv_refresh.sh'
    //   }
    // }

    // stage('Export Data') {
    //   steps {
    //     script{
    //       dir('data-warehouse/postgresql'){
    //         withCredentials([
    //           string(credentialsId: 'postgres_user', variable: 'POSTGRES_USER'),
    //           ]) {
    //           echo "#################### Export Data ####################"
    //           sh '''
    //             pwd
    //             mkdir -pv export
    //             ls -ld export

    //             docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_trip_user_year_hour) TO STDOUT WITH CSV HEADER" > export/mv_trip_user_year_hour.csv
    //             docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_trip_user_year_month) TO STDOUT WITH CSV HEADER" > export/mv_trip_user_year_month.csv
    //             docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_top_station_user_year) TO STDOUT WITH CSV HEADER" > export/mv_top_station_user_year.csv
    //             docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_station_year) TO STDOUT WITH CSV HEADER" > export/mv_station_year.csv
    //             docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_bike_year) TO STDOUT WITH CSV HEADER" > export/mv_bike_year.csv

    //           '''
    //           }
    //       }
    //     }
    //   }
    // }

    // stage('Upload to S3') {
    //   steps {
    //     dir('data-warehouse/postgresql'){
    //       script{
    //         echo "#################### Upload to S3 ####################"
    //         sh '''
    //           pwd
    //           ls ./export
    //         '''

    //         withAWS(credentials: 'aws_cred', region: 'ca-central-1') {
    //           s3Upload(
    //             bucket: 'toronto-shared-bike-data-warehouse-data-bucket', 
    //             path: 'test', 
    //             workingDir: 'export',
    //             includePathPattern:'**/*.csv',
    //           )
    //         }
    //       }
    //     }
    //   }
    // }
  }

  post {
    always {
      echo "#################### Cleanup PGDB ####################"
      dir("data-warehouse/postgresql"){
        sh 'docker compose -f docker-compose.yaml down -v || true'
      }

      echo "#################### Cleanup Workspace ####################"
      cleanWs()
    }
    
    failure {
      emailext (
        to: "tech.arguswatcher@gmail.com",
        subject: "FAILURE - ${env.JOB_NAME} (#${env.BUILD_NUMBER})",
        body: "Jenkins pipeline: '${env.JOB_NAME}'\n" +
          "Status: FAILURE \n" +
          "Build URL: ${env.BUILD_URL}"
      )
    }

    success {
      emailext (
        to: "tech.arguswatcher@gmail.com",
        subject: "SUCCESS - ${env.JOB_NAME} (#${env.BUILD_NUMBER})",
        body: "Jenkins pipeline: '${env.JOB_NAME}'\n" +
          "Status: SUCCESS \n" +
          "Build URL: ${env.BUILD_URL}"
      )
    }
  }
}