pipeline {
  agent any

  environment {
    POSTGRES_USER = 'postgres'
    POSTGRES_PASSWORD = 'SecurePassword123'
    POSTGRES_DB = 'toronto_shared_bike'
    PGDATA = '/var/lib/postgresql/data/pgdata'
  }

  stages {
    // stage('Copy Raw Data') {
    //   steps {
    //     sh '''
    //     cp -rv /data data-warehouse/postgresql/data
    //     ls data-warehouse/postgresql/data
    //     '''
    //   }s
    // }
    stage('Start PostgreSQL') {
      steps {
        sh '''
        docker compose -f data-warehouse/postgresql/docker-compose.yaml down -v
        docker compose -f data-warehouse/postgresql/docker-compose.yaml up -d --build
        docker logs postgresql
        '''
      }
    }

    // stage('Extract Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/extract.sh
    //     '''
    //   }
    // }
    // stage('Transform Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/transform.sh
    //     '''
    //   }
    // }
    stage('Load Data') {
      steps {
        sh '''
        docker exec -t postgresql bash /scripts/etl/load.sh
        '''
      }
    }
    stage('Refresh MV') {
      steps {
        sh '''
        docker exec -t postgresql bash /scripts/mv/mv_refresh.sh
        '''
      }
    }
    stage('Export Data') {
      steps {
        sh '''
        docker exec -t postgresql bash /scripts/export/export.sh
        '''
      }
    }
    stage('Upload Data') {
      steps {
        sh 'echo Upload'
      }
    }
  }

  // post {
  //   always {
  //     sh '''
  //     docker compose -f data-warehouse/postgresql/docker-compose.yaml down -v
  //     '''
  //   }
  //   success {
  //     echo 'Pipeline succeeded! Sending success notification.'
  //   // mail to: 'devs@example.com', subject: 'Pipeline Success'
  //   }
  //   failure {
  //     echo 'Pipeline failed! Sending failure notification.'
  //   // mail to: 'devs@example.com', subject: 'Pipeline Failure'
  //   }
  // }
}
