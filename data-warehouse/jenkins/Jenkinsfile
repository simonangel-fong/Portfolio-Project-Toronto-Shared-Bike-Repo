pipeline {
  agent any

  options {
    // limit only one instance
    disableConcurrentBuilds()
    // show timestamp for event
    timestamps()
    // discard old builds
    buildDiscarder(
      logRotator(
        // maximum number of the last build logs
        numToKeepStr: '10',   
        // maximum number of the last sets of build artifacts
        artifactNumToKeepStr: '5'   
      )
    )
    // set a timeout for the entire pipeline run
    timeout(time: 60, unit: 'MINUTES')

    //Prevents Jenkins from automatically checking out the SCM.
    skipDefaultCheckout(true)
  }

  triggers {
    cron 'H 4 * * *'    // every 4 a.m.
  }

  environment {
    // POSTGRES_USER = 'postgres'
    // POSTGRES_PASSWORD = 'SecurePassword123'
    POSTGRES_DB = 'toronto_shared_bike'
    PGDATA = '/var/lib/postgresql/data/pgdata'

    AWS_REGION = 'ca-central-1'
    S3_CREDENTIAL = 'toronto_shared_bike'
    POSTGRES_DIR        = '/var/lib/jenkins/workspace/etl-pipeline/data-warehouse/postgresql'
    POSTGRES_DATA_DIR   = '/var/lib/jenkins/workspace/etl-pipeline/data-warehouse/postgresql/data'
    POSTGRES_EXPORT_DIR = '/var/lib/jenkins/workspace/etl-pipeline/data-warehouse/postgresql/export'
    POSTGRES_DOCKE_FILE = '/var/lib/jenkins/workspace/etl-pipeline/data-warehouse/postgresql/docker-compose.yaml'
    CSV_URL = 'https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/data.zip'
    S3_BUCKET  = 'toronto-shared-bike-data-warehouse-data-bucket'
    S3_BUCKET_PREFIX  = 'test'
    EMAIL = "tech.arguswatcher@gmail.com"

  }

  stages {

    // stage('Cleanup Workspace & Checkout') {
    //     steps {
    //         echo "#################### Cleans the workspace ####################"
    //         cleanWs()
            
    //         echo "#################### Checkout ####################"
    //         checkout scm // Explicitly checks out the SCM after cleaning.
    //     }
    // }

    stage('Download CSV Files') {
      steps {
        echo "#################### Download CSV zip ####################"
        sh '''
          rm -rf data-warehouse/postgresql/data
          mkdir -pv data-warehouse/postgresql/data
          ls -dl data-warehouse/postgresql/data

          curl -o data-warehouse/postgresql/csv.zip https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/data.zip
          ls -l data-warehouse/postgresql/csv.zip
        '''
          
        echo "#################### Unzip CSV zip ####################"
        sh '''
          unzip -o data-warehouse/postgresql/csv.zip -d data-warehouse/postgresql
          ls -l data-warehouse/postgresql/data
          du -h data-warehouse/postgresql/data

          rm -fv data-warehouse/postgresql/csv.zip
        '''
      }
    }

    stage('Start PostgreSQL') {
      steps {
          
          echo "#################### Spin up PGDB ####################"
          sh '''
            docker compose -f data-warehouse/postgresql/docker-compose.yaml down -v
            docker compose -f data-warehouse/postgresql/docker-compose.yaml up -d --build

            # Wait until Postgres is ready
            until docker exec -t postgresql bash -lc 'pg_isready -U "${POSTGRES_USER:-postgres}" -d "${POSTGRES_DB:-toronto_shared_bike}"'; do
              echo "Waiting for PostgreSQL to become ready..."
              sleep 10
            done
          '''
          
          echo "#################### Confirm PGDB ####################"
          sh '''
            docker ps
            docker logs --tail=100 postgresql || true
          '''
      }
    }

    stage('Extract Data') {
      steps {
        echo "#################### Extract Data ####################"
        sh '''
          docker exec -t postgresql bash /scripts/etl/extract.sh
        '''
      }
    }

    // stage('Transform Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/transform.sh
    //     '''
    //   }
    // }

    // stage('Load Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/load.sh
    //     '''
    //   }
    // }

    // stage('Refresh Materialized Views') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/mv/mv_refresh.sh
    //     '''
    //   }
    // }

    // stage('Export Data') {
    //   steps {
    //     echo "#################### Export Data ####################"
    //     sh '''
    //       rm -rf $POSTGRES_EXPORT_DIR
    //       mkdir -pv $POSTGRES_EXPORT_DIR
    //       ls -ld $POSTGRES_EXPORT_DIR

    //       docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_trip_user_year_hour) TO STDOUT WITH CSV HEADER" > $POSTGRES_EXPORT_DIR/mv_trip_user_year_hour.csv
    //       docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_trip_user_year_month) TO STDOUT WITH CSV HEADER" > $POSTGRES_EXPORT_DIR/mv_trip_user_year_month.csv
    //       docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_top_station_user_year) TO STDOUT WITH CSV HEADER" > $POSTGRES_EXPORT_DIR/mv_top_station_user_year.csv
    //       docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_station_year) TO STDOUT WITH CSV HEADER" > $POSTGRES_EXPORT_DIR/mv_station_year.csv
    //       docker exec -t postgresql psql -U $POSTGRES_USER -d $POSTGRES_DB -c "COPY (SELECT * FROM dw_schema.mv_bike_year) TO STDOUT WITH CSV HEADER" > $POSTGRES_EXPORT_DIR/mv_bike_year.csv
    //     '''
    //   }
    // }

    // stage('Upload to S3') {
    //   steps {
    //     script{
    //       withAWS(credentials: env.S3_CREDENTIAL, region: env.AWS_REGION) {
    //         s3Upload(
    //           bucket: env.S3_BUCKET, 
    //           path: env.S3_BUCKET_PREFIX, 
    //           workingDir: env.POSTGRES_EXPORT_DIR,
    //           includePathPattern:'**/*.csv',
    //         )
    //       }
    //     }
    //   }
    // }
  }

  post {
    always {
      echo "#################### Cleanup PGDB ####################"
      sh '''
      docker compose -f data-warehouse/postgresql/docker-compose.yaml down
      '''

      echo "#################### Cleanup Workspace ####################"
      // cleanWs()
    }
    
    failure {
      emailext (
        to: "tech.arguswatcher@gmail.com",
        subject: "FAILURE: ${env.JOB_NAME} - Build #${env.BUILD_NUMBER}",
        body: "The Jenkins pipeline '${env.JOB_NAME}' failed.\n" +
          "Build URL: ${env.BUILD_URL}",
      )
    }

    success {
      emailext (
        to: "tech.arguswatcher@gmail.com",
        subject: "SUCCESS: ${env.JOB_NAME} - Build #${env.BUILD_NUMBER}",
        body: "The Jenkins pipeline '${env.JOB_NAME}' completed successfully.\n" +
          "Build URL: ${env.BUILD_URL}",
      )
    }
  }
}