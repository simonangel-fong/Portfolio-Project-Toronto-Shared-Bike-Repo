pipeline {
  agent any

  environment {
    POSTGRES_USER = 'postgres'
    POSTGRES_PASSWORD = 'SecurePassword123'
    POSTGRES_DB = 'toronto_shared_bike'
    PGDATA = '/var/lib/postgresql/data/pgdata'
    S3_PROFILE = 'toronto_shared_bike'
    S3_BUCKET  = 'toronto-shared-bike-data-warehouse-data-bucket'
    S3_PREFIX  = "mv" 
    EXPORT_DIR = '/export'
  }

  stages {
    // stage('Copy Raw Data') {
    //   steps {
    //     sh '''
    //     cp -rv /data data-warehouse/postgresql/data
    //     ls data-warehouse/postgresql/data
    //     '''
    //   }s
    // }
    // stage('Start PostgreSQL') {
    //   steps {
    //     sh '''
    //     docker compose -f data-warehouse/postgresql/docker-compose.yaml up -d --build
    //     sleep 30
    //     docker logs postgresql
    //     '''
    //   }
    // }

    // stage('Extract Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/extract.sh
    //     '''
    //   }
    // }
    // stage('Transform Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/transform.sh
    //     '''
    //   }
    // }
    // stage('Load Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/etl/load.sh
    //     '''
    //   }
    // }
    // stage('Refresh MV') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/mv/mv_refresh.sh
    //     '''
    //   }
    // }
    // stage('Export Data') {
    //   steps {
    //     sh '''
    //     docker exec -t postgresql bash /scripts/export/export.sh
    //     '''
    //   }
    // }
    stage('Upload to S3') {
      steps {
        script {
          // withAWS(region: 'ca-central-1', credentials: 'toronto_shared_bike') {
            // sh 'aws s3 ls'
            // s3Upload(
            //   bucket: 'toronto-shared-bike-data-warehouse-data-bucket',
            //   file: 'mv_bike_year.csv',
            //   path: 'test/', 
            //   acl: 'Private', // Or 'PublicRead', etc.
            //   workingDir: '/export' // Directory where the artifact is located relative to workspace
            // )
          // }
          sh '''
          aws --version
          '''
        }
      }
    }
  }

  // post {
  //   always {
  //     sh '''
  //     docker compose -f data-warehouse/postgresql/docker-compose.yaml down -v
  //     '''
  //   }
  //   success {
  //     echo 'Pipeline succeeded! Sending success notification.'
  //   // mail to: 'devs@example.com', subject: 'Pipeline Success'
  //   }
  //   failure {
  //     echo 'Pipeline failed! Sending failure notification.'
  //   // mail to: 'devs@example.com', subject: 'Pipeline Failure'
  //   }
  // }
}
