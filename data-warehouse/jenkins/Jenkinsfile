pipeline {
  agent any

  options {
    // limit only one instance
    disableConcurrentBuilds()
    // show timestamp for event
    timestamps()
    // discard old builds
    buildDiscarder(
      logRotator(
        // maximum number of the last build logs
        numToKeepStr: '10',   
        // maximum number of the last sets of build artifacts
        artifactNumToKeepStr: '5'   
      )
    )
    // set a timeout for the entire pipeline run
    timeout(time: 60, unit: 'MINUTES')

    //Prevents Jenkins from automatically checking out the SCM.
    skipDefaultCheckout(true)
  }

  triggers {
    cron 'H H/4 * * *'    // run every four hours
  }

  environment {
    POSTGRES_USER = 'postgres'
    POSTGRES_PASSWORD = 'SecurePassword123'
    POSTGRES_DB = 'toronto_shared_bike'
    PGDATA = '/var/lib/postgresql/data/pgdata'
    // S3_PROFILE = 'toronto_shared_bike'
    // S3_BUCKET  = 'toronto-shared-bike-data-warehouse-data-bucket'
    // RAW_URL = 'https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/data.zip'
    // S3_PREFIX  = "mv" 
    // EXPORT_DIR = '/export'
    
    // COMPOSE_PROJECT_NAME = "${env.JOB_NAME}-${env.BUILD_NUMBER}"
    // PRJECT_NAME = 'toronto_shared_bike'
    // EXPORT_DIR = '/export'
    // AWS_REGION = 'ca-central-1'
    // S3_CREDENTIAL = 'toronto_shared_bike'
    POSTGRES_DIR        = 'data-warehouse/postgresql'
    POSTGRES_DATA_DIR   = 'data-warehouse/postgresql/data'
    POSTGRES_EXPORT_DIR = 'data-warehouse/postgresql/export'
    POSTGRES_DOCKE_FILE = 'data-warehouse/postgresql/docker-compose.yaml'
    CSV_URL = 'https://toronto-shared-bike-data-warehouse-data-bucket.s3.ca-central-1.amazonaws.com/raw/data.zip'

  }

  stages {

    stage('Cleanup Workspace & Checkout') {
        steps {
            echo "#################### Cleans the workspace ####################"
            cleanWs()
            
            echo "#################### Checkout ####################"
            checkout scm // Explicitly checks out the SCM after cleaning.
        }
    }

    stage('Download CSV Files') {
      steps {
        echo "#################### Download CSV zip ####################"
        sh '''
          pwd
          ls -l
          mkdir -pv $POSTGRES_DATA_DIR
          curl -o $POSTGRES_DATA_DIR/csv.zip $CSV_URL
          cd $POSTGRES_DATA_DIR
          pwd
          ls -l
        '''
          
        echo "#################### Unzip CSV zip ####################"
        sh '''
          cd $POSTGRES_DATA_DIR
          pwd
          ls -l
          unzip -o $POSTGRES_DATA_DIR/csv.zip -d $POSTGRES_DIR
          rm -f $POSTGRES_DATA_DIR/csv.zip
          pwd
          ls -l
        '''

        echo "#################### Confirm ####################"
        sh '''
          ls -l $POSTGRES_DATA_DIR
          pwd
          du -h $POSTGRES_DATA_DIR
        '''
      }
    }

    // stage('Start PostgreSQL') {
    //   steps {
    //     dir(env.POSTGRES_DIR) {
          
    //       echo "#################### Spin up PGDB ####################"
    //       sh '''
    //         docker compose -f docker-compose.yaml down
    //         docker compose -f docker-compose.yaml up -d --build

    //         # Wait until Postgres is ready instead of sleeping blindly
    //         until docker exec -t postgresql bash -lc 'pg_isready -U "${POSTGRES_USER:-postgres}" -d "${POSTGRES_DB:-toronto_shared_bike}"'; do
    //           echo "Waiting for PostgreSQL to become ready..."
    //           sleep 3
    //         done
    //       '''
          
    //       echo "#################### Confirm PGDB ####################"
    //       sh '''
    //         docker ps
    //         docker logs --tail=100 postgresql || true
    //       '''
    //     }
    //   }
    // }

    // stage('Extract Data') {
    //   steps {
    //     echo "#################### Extract Data ####################"
    //     sh '''
    //       docker exec -t postgresql bash /scripts/etl/extract.sh
    //     '''
    //   }
    // }

  //   stage('Transform Data') {
  //     steps {
  //       sh '''
  //       docker exec -t postgresql bash /scripts/etl/transform.sh
  //       '''
  //     }
  //   }

  //   stage('Load Data') {
  //     steps {
  //       sh '''
  //       docker exec -t postgresql bash /scripts/etl/load.sh
  //       '''
  //     }
  //   }

  //   stage('Refresh Materialized Views') {
  //     steps {
  //       sh '''
  //       docker exec -t postgresql bash /scripts/mv/mv_refresh.sh
  //       '''
  //     }
  //   }

    // stage('Export Data') {
    //   steps {
    //     echo "#################### Export Data ####################"
    //     sh '''
    //       docker exec -t postgresql bash /scripts/export/export.sh
    //     '''
    //   }
    // }

  //   stage('Upload to S3') {
  //     steps {
  //       script{
  //         withAWS(credentials: env.S3_CREDENTIAL, region: env.AWS_REGION) {
  //           s3Upload(
  //             bucket:"toronto-shared-bike-data-warehouse-data-bucket", 
  //             path:'test', 
  //             workingDir:'/export',
  //             includePathPattern:'**/*.csv',
  //           )
  //         }
  //       }
  //     }
  //   }
  }

  // post {
    // always {
    //   echo "#################### Cleanup PGDB ####################"
    //   sh '''
    //   # docker compose -f data-warehouse/postgresql/docker-compose.yaml down -v
    //   docker volume ls
    //   docker volume prune -f
    //   '''

    //   echo "#################### Cleanup Workspace ####################"
    //   // cleanWs()
    // }
    
  //   failure {
  //     emailext (
  //       to: "tech.arguswatcher@gmail.com",
  //       subject: "FAILURE: ${env.JOB_NAME} - Build #${env.BUILD_NUMBER}",
  //       body: "The Jenkins pipeline '${env.JOB_NAME}' failed.\n" +
  //         "Build URL: ${env.BUILD_URL}",
  //     )
  //   }

  //   success {
  //     emailext (
  //       to: "tech.arguswatcher@gmail.com",
  //       subject: "SUCCESS: ${env.JOB_NAME} - Build #${env.BUILD_NUMBER}",
  //       body: "The Jenkins pipeline '${env.JOB_NAME}' completed successfully.\n" +
  //         "Build URL: ${env.BUILD_URL}",
  //     )
  //   }
  // }
}
